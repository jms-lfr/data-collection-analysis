# Social Media Data Collection and Analysis
For this project, we collected posts and comments from social media sites (Reddit and 4chan) and analyzed them for toxicity, and performed some data analysis on the results. Specifically, we evaluated the level of toxicity among users of video game-related forums, determined which site and subforums generated the most toxic content, and looked for noteworthy trends or patterns.  

## Collection
To gather data from Reddit subreddits, we utilized Reddit's `comments.json` and `new.json` API endpoints with query parameters to get the maximum number of comments/posts possible and to sort by new. Then, using a binary search, we found the earliest comment/post that hadn't been added to our database, and added that comment/post plus everything that came after it. To crawl Reddit, we created a Faktory job producing client that submits a job every few seconds to either get subreddit posts, comments, or both, depending on what was done last. We did not want to always get both because in general, there are more new comments created than new original posts. When the worker gets the job, it makes a get request to Reddit, finds the earliest post in the response that hasn't been added to our database (using a binary search), then adds that post/comment and everything that came after it. Our job producing client also tells the worker to update its OAuth key every 24 hours, but the worker can update the key independently if it receives a 401 response from Reddit.  

The 4chan implementation is much more complicated than the Reddit implementation. To collect 4chan posts, we utilized the official 4chan API's `{board}/catalog.json` and `{board}/thread/{thread_id}.json` endpoints. Using Faktory, each board and thread in our "threads to crawl" set were crawled every 30 seconds. Each thread is crawled together with, in the ideal case, 49 other threads in a process, with a maximum of 25 processes, leading to each thread being crawled every ~50 seconds. The Faktory job producer submits the next board to crawl every few seconds so that each board is crawled every (about) 30 seconds. The catalog worker gets the catalog of the given board and adds all of its threads to the set of threads being crawled. If it's already in the set, then it's already being crawled and doesn't have to be queued. Otherwise, it's added to the set of new threads to crawl. After going through the whole catalog, the worker queues a job to the thread crawler consisting of the board to crawl and the new threads list. 

When the thread crawler receives a list of threads to crawl, it splits the list into groups of 50 and creates a new process for the group (max 25 processes), which crawls a thread every few seconds to end up crawling each thread every (about) 50 seconds. (In total, being able to crawl 1250 (50*25) threads at once should be enough to crawl everything in the relevant 4chan boards.) When a thread dies, is closed, or is otherwise made impossible to post in, the thread is removed from the thread crawler's list of threads to crawl. In addition, it submits a job to the catalog crawler that removes it from the set containing threads being crawled. This is necessary because if the program ran for long enough, that set of threads could use up an unnecessarily large amount of memory. Additionally, the created thread crawling process ends when the amount of threads it's crawling is less than half of the amount that it started with so we can crawl as many threads as possible with fewer processes. In the ending process, the job to remove threads from the "threads being crawled" set is queued, but if we still need to crawl these threads, they'll still be in the 4chan catalog and thus be re-added back to the set and re-queued to the thread crawler.  

## Analysis (individual comments)
After collecting our Reddit and 4chan data, both groups of data are analyzed using the `ToxBlock` Python library and ModerateHatespeech.com (MHS). The ToxBlock analysis script is simple: first, it gets a maximum of 200 not-yet-analyzed Reddit and 4chan posts (maximum of 100 from each). Then, it analyzes the text content, gets each post's `toxicity` probability, and updates each comment's respective `toxicity_rating` field in the database with the value. Analyzing with MHS is done through their API, which does not support batch-analysis like ToxBlock, but otherwise the process is similar to that of ToxBlock, but with more error handling.  

## Challenges
One challenge we had while working on this project was that the ModerateHatespeech (MHS) API has a considerable amount of undocumented behavior. For example, certain strings cause MHS to return an error rather than a successful response and toxicity score. Examples of these strings (case-sensitive, exact) are "0" and "Skill issue", the latter being a fairly common response in gaming communities. MHS also returns its "confidence" in its result ("normal" or "flag") as a string rather than an int/float, which we then have to cast, which is slightly annoying. Finally, while we were running our collection and analysis, MHS would inconsistently go down for unknown reasons, which prevented us from doing non-ToxBlock analysis.  

Our largest issue was that the VM we were running our implementation on would randomly crash. We initially thought this was a problem with our implementation (possibly using too much memory, for example), but in some cases, the VM would crash even when it was just idling. The crash would close our PostgreSQL and our Faktory servers, and if we left our data collection code running, it would also stop and have to be restarted. We (and our professor) could not figure out why the VM would crash, so we improvised: we created cron jobs that would run everything we needed on boot, so after a crash, everything would automatically restart. Oddly enough, after starting the VM after a power outage at the school, our crashing problem disappeared.
